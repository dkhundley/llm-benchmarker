{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17958130",
   "metadata": {},
   "source": [
    "# MMLU - Massive Multitask Language Understanding\n",
    "\n",
    "MMLU stands for **Massive Multitask Language Understanding**, and it is perhaps the most popular metric used across model cards to demonstrate a model's performance in terms of knowledge breadth. This benchmark contains a series of scenarios and questions for the LLM to answer across 57 different domains. These domains include STEM, humanities, social sciences, and more. Within each of these domains, there include questions that range from more generalized areas, like history of the topic, and then there are questions that are more specialized in nature or ask \"harder\" questions, like ethical implications.\n",
    "\n",
    "Originally conceived by a team a UC Berkeley in ?, MMLU has evolved into many different flavors, each taking variance on things like prompting style, evaluation codes, or even using a subset of all the questions asked. HuggingFace has [a really great write up](https://github.com/huggingface/blog/blob/main/evaluating-mmlu-leaderboard.md) on all these different variations, and while they all can produce a wide range of differences, the same goal remains: assessing the LLM's breadth of knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da226943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c719436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading API keys from file (NOT pushed up to GitHub)\n",
    "with open('../keys/api_keys.yaml') as f:\n",
    "    API_KEYS = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4653a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBCATEGORIES = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "CHOICES = ['A', 'B', 'C', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa6e708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Down)loading the MMLU dataset from HuggingFace\n",
    "mmlu_dataset = load_dataset(path = 'cais/mmlu',\n",
    "                            name = 'all',\n",
    "                            cache_dir = '../data/',\n",
    "                            trust_remote_code = True,\n",
    "                            split = 'dev')\n",
    "\n",
    "# Loading the dataset as a Pandas dataframe\n",
    "df_mmlu = mmlu_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d794629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of all the subjects\n",
    "subjects = sorted(df_mmlu['subject'].value_counts().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42def43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following best describes the bala...</td>\n",
       "      <td>high_school_government_and_politics</td>\n",
       "      <td>[Freedom of speech is protected except in cert...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which of the following statements does NOT acc...</td>\n",
       "      <td>high_school_government_and_politics</td>\n",
       "      <td>[Registered voters between the ages of 35 and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following plays the most signific...</td>\n",
       "      <td>high_school_government_and_politics</td>\n",
       "      <td>[The geographical area in which the child grow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What power was granted to the states by the Ar...</td>\n",
       "      <td>high_school_government_and_politics</td>\n",
       "      <td>[Coining money, Authorizing constitutional ame...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The primary function of political action commi...</td>\n",
       "      <td>high_school_government_and_politics</td>\n",
       "      <td>[contribute money to candidates for election, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Which of the following best describes the bala...   \n",
       "1  Which of the following statements does NOT acc...   \n",
       "2  Which of the following plays the most signific...   \n",
       "3  What power was granted to the states by the Ar...   \n",
       "4  The primary function of political action commi...   \n",
       "\n",
       "                               subject  \\\n",
       "0  high_school_government_and_politics   \n",
       "1  high_school_government_and_politics   \n",
       "2  high_school_government_and_politics   \n",
       "3  high_school_government_and_politics   \n",
       "4  high_school_government_and_politics   \n",
       "\n",
       "                                             choices  answer  \n",
       "0  [Freedom of speech is protected except in cert...       3  \n",
       "1  [Registered voters between the ages of 35 and ...       1  \n",
       "2  [The geographical area in which the child grow...       1  \n",
       "3  [Coining money, Authorizing constitutional ame...       0  \n",
       "4  [contribute money to candidates for election, ...       0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mmlu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d766c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'subject', 'choices', 'answer'],\n",
       "    num_rows: 14042\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd62dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
