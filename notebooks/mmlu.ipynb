{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17958130",
   "metadata": {},
   "source": [
    "# MMLU - Massive Multitask Language Understanding\n",
    "\n",
    "MMLU stands for **Massive Multitask Language Understanding**, and it is perhaps the most popular metric used across model cards to demonstrate a model's performance in terms of knowledge breadth. This benchmark contains a series of scenarios and questions for the LLM to answer across 57 different domains. These domains include STEM, humanities, social sciences, and more. Within each of these domains, there include questions that range from more generalized areas, like history of the topic, and then there are questions that are more specialized in nature or ask \"harder\" questions, like ethical implications.\n",
    "\n",
    "Originally conceived by a team a UC Berkeley in ?, MMLU has evolved into many different flavors, each taking variance on things like prompting style, evaluation codes, or even using a subset of all the questions asked. HuggingFace has [a really great write up](https://github.com/huggingface/blog/blob/main/evaluating-mmlu-leaderboard.md) on all these different variations, and while they all can produce a wide range of differences, the same goal remains: assessing the LLM's breadth of knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da226943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c719436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading API keys from file (NOT pushed up to GitHub)\n",
    "with open('../keys/api_keys.yaml') as f:\n",
    "    API_KEYS = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4653a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBCATEGORIES = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "CHOICES = ['A', 'B', 'C', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6e708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Down)loading the MMLU dataset from HuggingFace\n",
    "mmlu_dataset = load_dataset(path = 'cais/mmlu',\n",
    "                            name = 'all',\n",
    "                            cache_dir = '../data/',\n",
    "                            trust_remote_code = True,\n",
    "                            split = 'dev')\n",
    "\n",
    "# Loading the dataset as a Pandas dataframe\n",
    "df_mmlu = mmlu_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d794629c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'business_ethics'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a list of all the subjects\n",
    "subjects = sorted(df_mmlu['subject'].value_counts().keys())\n",
    "subject = subjects[3]\n",
    "formatted_subject = subject.replace('_', ' ')\n",
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbf47ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'business ethics'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dd62dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subject = df_mmlu[df_mmlu['subject'] == subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0336d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_START = \"The following are questions (with answers) about {}.\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95a3f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_MMLU_PROMPT = '''The following are questions (with answers) about {}.\n",
    "\n",
    "{}\n",
    "{}\n",
    "Answer: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e2b4a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are questions (with answers) about business ethics.\n",
      "\n",
      "question\n",
      "choices\n",
      "Answer: my_answer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ORIGINAL_MMLU_PROMPT.format(formatted_subject, 'question', 'choices', 'my_answer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a1bd1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are questions (with answers) about business ethics.\n",
      "\n",
      "Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, the _______that corporations possess and the ________ of business and society.\n",
      "A. Externalities, Power, Independence\n",
      "B. Publicity, Insubstantial resources, Mutual dependence\n",
      "C. Publicity, Power, Independence\n",
      "D. Externalities, Power, Mutual dependence\n",
      "\n",
      "Answer: D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The following are questions (with answers) about business ethics.\n",
      "\n",
      "_______ is the direct attempt to formally or informally manage ethical issues or problems, through specific policies, practices and programmes.\n",
      "A. Corporate social responsibility\n",
      "B. Business ethics management\n",
      "C. Sustainability\n",
      "D. Environmental management\n",
      "\n",
      "Answer: D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The following are questions (with answers) about business ethics.\n",
      "\n",
      "To ensure the independence of the non-executive board members, they are a number of steps which can be taken, which include non-executives being drawn from _______ the company, being appointed for a _________ time period as well as being appointed _________.\n",
      "A. Outside, Limited, Independently\n",
      "B. Inside, Limited, Intermittently\n",
      "C. Outside, Unlimited, Intermittently\n",
      "D. Inside, Unlimited, Independently\n",
      "\n",
      "Answer: D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The following are questions (with answers) about business ethics.\n",
      "\n",
      "Three contrasting tactics that CSO's can engage in to meet their aims are ________ which typically involves research and communication, ________, which may involve physically attacking a company's operations or ________, often involving some form of _______.\n",
      "A. Non-violent direct action, Violent direct action, Indirect action, Boycott\n",
      "B. Indirect action, Instrumental action, Non-violent direct action, Information campaign\n",
      "C. Indirect action, Violent direct action, Non-violent direct-action Boycott\n",
      "D. Non-violent direct action, Instrumental action, Indirect action, Information campaign\n",
      "\n",
      "Answer: D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The following are questions (with answers) about business ethics.\n",
      "\n",
      "In contrast to _______, _______ aim to reward favourable behaviour by companies. The success of such campaigns have been heightened through the use of ___________, which allow campaigns to facilitate the company in achieving _________ .\n",
      "A. Buycotts, Boycotts, Blockchain technology, Charitable donations\n",
      "B. Buycotts, Boycotts, Digital technology, Increased Sales\n",
      "C. Boycotts, Buyalls, Blockchain technology, Charitable donations\n",
      "D. Boycotts, Buycotts, Digital technology, Increased Sales\n",
      "\n",
      "Answer: D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterating over all the rows of the DataFrame\n",
    "for row in range(len(df_subject)):\n",
    "    \n",
    "    # Extracting the question from the row\n",
    "    question = df_subject.iloc[row]['question']\n",
    "\n",
    "    # Extracting the choices from the row\n",
    "    choices = ''\n",
    "    all_choices = df_subject.iloc[row]['choices']\n",
    "    for index, choice in enumerate(all_choices):\n",
    "        choices += f'{CHOICES[index]}. {choice}\\n'\n",
    "\n",
    "    # Extracting the answer from the row\n",
    "    number_answer = df_subject.iloc[0]['answer']\n",
    "    answer = CHOICES[number_answer]\n",
    "\n",
    "    # Formatting the prompt with the derived information\n",
    "    prompt = ORIGINAL_MMLU_PROMPT.format(formatted_subject, question, choices, answer)\n",
    "\n",
    "    print(prompt)\n",
    "    print('\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc9f25",
   "metadata": {},
   "source": [
    "Typical advertising regulatory bodies suggest, for example that adverts must not: encourage _________, cause unnecessary ________ or _____, and must not cause _______ offence.\n",
    "\n",
    "A. Unsafe practices, Wants, Fear, Trivial\n",
    "\n",
    "B. Unsafe practices, Distress, Fear, Serious\n",
    "\n",
    "C. Safe practices, Wants, Jealousy, Trivial\n",
    "\n",
    "D. Safe practices, Distress, Jealousy, Serious\n",
    "\n",
    "Answer: B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51f79696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_answer = df_subject.iloc[0]['answer']\n",
    "answer = CHOICES[number_answer]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "429c0ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. Buycotts, Boycotts, Blockchain technology, Charitable donations\n",
      "B. Buycotts, Boycotts, Digital technology, Increased Sales\n",
      "C. Boycotts, Buyalls, Blockchain technology, Charitable donations\n",
      "D. Boycotts, Buycotts, Digital technology, Increased Sales\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choices = ''\n",
    "for index, choice in enumerate(all_choices):\n",
    "    choices += f'{CHOICES[index]}. {choice}\\n'\n",
    "print(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deaeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
