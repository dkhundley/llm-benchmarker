{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17958130",
   "metadata": {},
   "source": [
    "# MMLU - Massive Multitask Language Understanding\n",
    "\n",
    "MMLU stands for **Massive Multitask Language Understanding**, and it is perhaps the most popular metric used across model cards to demonstrate a model's performance in terms of knowledge breadth. This benchmark contains a series of scenarios and questions for the LLM to answer across 57 different domains. These domains include STEM, humanities, social sciences, and more. Within each of these domains, there include questions that range from more generalized areas, like history of the topic, and then there are questions that are more specialized in nature or ask \"harder\" questions, like ethical implications.\n",
    "\n",
    "Originally conceived by a team a UC Berkeley in ?, MMLU has evolved into many different flavors, each taking variance on things like prompting style, evaluation codes, or even using a subset of all the questions asked. HuggingFace has [a really great write up](https://github.com/huggingface/blog/blob/main/evaluating-mmlu-leaderboard.md) on all these different variations, and while they all can produce a wide range of differences, the same goal remains: assessing the LLM's breadth of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24612808",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da226943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c719436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading API keys from file (NOT pushed up to GitHub)\n",
    "with open('../keys/api_keys.yaml') as f:\n",
    "    API_KEYS = yaml.safe_load(f)\n",
    "\n",
    "# Statically setting the choices available\n",
    "MMLU_CHOICES = ['A', 'B', 'C', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94934ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    z = x - max(x)\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator)\n",
    "    softmax = numerator/denominator\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03e567",
   "metadata": {},
   "source": [
    "## LLM & LangChain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f2ce925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the OpenAI client with LangChain\n",
    "llm = ChatOpenAI(\n",
    "    api_key = API_KEYS['OPENAI_API_KEY'],\n",
    "    model_name = 'gpt-4',\n",
    "    temperature = 0,\n",
    "    max_tokens = 1,\n",
    "    model_kwargs = {\n",
    "        'logprobs': True,\n",
    "        'top_logprobs': 5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0641222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing the original prompt\n",
    "ORIGINAL_MMLU_PROMPT_TEMPLATE = '''The following are questions (with answers) about {formatted_subject}.\n",
    "\n",
    "{question}\n",
    "{choices}\n",
    "'''\n",
    "\n",
    "# Setting a template around the prompt\n",
    "mmlu_prompt_template = HumanMessagePromptTemplate.from_template(ORIGINAL_MMLU_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d367f9",
   "metadata": {},
   "source": [
    "## Loading the MMLU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd64871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Down)loading the MMLU dataset from HuggingFace\n",
    "mmlu_dataset = load_dataset(path = 'cais/mmlu',\n",
    "                            name = 'all',\n",
    "                            cache_dir = '../data/',\n",
    "                            trust_remote_code = True,\n",
    "                            split = 'dev')\n",
    "\n",
    "# Loading the dataset as a Pandas dataframe\n",
    "df_mmlu = mmlu_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788f55f",
   "metadata": {},
   "source": [
    "## Starting With One Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d794629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: abstract_algebra\n",
      "Formatted Subject: abstract algebra\n"
     ]
    }
   ],
   "source": [
    "# Getting a list of all the subjects\n",
    "subjects = sorted(df_mmlu['subject'].value_counts().keys())\n",
    "\n",
    "# Extracting a single sample subject\n",
    "subject = subjects[0]\n",
    "\n",
    "# Formatting the subject by removing the underscores\n",
    "formatted_subject = subject.replace('_', ' ')\n",
    "\n",
    "print(f'Subject: {subject}')\n",
    "print(f'Formatted Subject: {formatted_subject}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd62dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Find all c in Z_3 such that Z_3[x]/(x^2 + c) i...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Statement 1 | If aH is an element of a factor ...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[True, True, False, False, True, False, False,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Statement 1 | Every element of a group generat...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[True, True, False, False, True, False, False,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Statement 1| Every function from a finite set ...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[True, True, False, False, True, False, False,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Find the characteristic of the ring 2Z.</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[0, 3, 12, 30]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question           subject  \\\n",
       "125  Find all c in Z_3 such that Z_3[x]/(x^2 + c) i...  abstract_algebra   \n",
       "126  Statement 1 | If aH is an element of a factor ...  abstract_algebra   \n",
       "127  Statement 1 | Every element of a group generat...  abstract_algebra   \n",
       "128  Statement 1| Every function from a finite set ...  abstract_algebra   \n",
       "129            Find the characteristic of the ring 2Z.  abstract_algebra   \n",
       "\n",
       "                                               choices  answer  \n",
       "125                                       [0, 1, 2, 3]       1  \n",
       "126  [True, True, False, False, True, False, False,...       1  \n",
       "127  [True, True, False, False, True, False, False,...       2  \n",
       "128  [True, True, False, False, True, False, False,...       0  \n",
       "129                                     [0, 3, 12, 30]       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering down the full dataset to only include the currently analyzed subject\n",
    "df_subject = df_mmlu[df_mmlu['subject'] == subject]\n",
    "df_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aef295fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': -0.9243025, 'Answer': -1.0649276, 'Z': -2.3774276, 'In': -2.5180526, 'A': -3.2836776}\n"
     ]
    }
   ],
   "source": [
    "# Instantiating containers to hold all boolean \"is correct\" values and probabilities for this subject\n",
    "subject_is_corrects = []\n",
    "subject_probs = []\n",
    "\n",
    "# Iterating over all the rows of the DataFrame\n",
    "for index, row in df_subject.iterrows():\n",
    "    \n",
    "    # Extracting the question from the row\n",
    "    question = row['question']\n",
    "\n",
    "    # Extracting the choices from the row\n",
    "    choices = ''\n",
    "    all_choices = row['choices']\n",
    "    for index, choice in enumerate(all_choices):\n",
    "        choices += f'{MMLU_CHOICES[index]}. {choice}\\n'\n",
    "\n",
    "    # Extracting the answer from the row\n",
    "    number_answer = row['answer']\n",
    "    ground_truth = MMLU_CHOICES[number_answer]\n",
    "\n",
    "    # Setting the variables within the template\n",
    "    messages = mmlu_prompt_template.format_messages(\n",
    "        formatted_subject = formatted_subject,\n",
    "        question = question,\n",
    "        choices = choices\n",
    "    )\n",
    "\n",
    "    # Invoking the LLM to generate a response\n",
    "    llm_response = json.loads(llm.generate(messages = [messages]).json())\n",
    "\n",
    "    # Extracting the prediction from the LLM response\n",
    "    prediction = llm_response['generations'][0][0]['text']\n",
    "\n",
    "    # Extracting the log probs from the LLM response\n",
    "    logprobs = llm_response['generations'][0][0]['generation_info']['logprobs']['content'][0]['top_logprobs']\n",
    "\n",
    "    # Instantiating a dictionary to a direct key-value pair between token name and that token's logprob\n",
    "    token_logprobs = {}\n",
    "\n",
    "    # Iterating over the \"raw\" log probs\n",
    "    for logprob in logprobs:\n",
    "        token_name = logprob['token']\n",
    "        token_logprob = logprob['logprob']\n",
    "        token_logprobs[token_name] = token_logprob\n",
    "\n",
    "    # Setting the softmax values for the logrobs\n",
    "    softmax(np.array(list(token_logprobs.values())))\n",
    "\n",
    "    # Creating a boolean value to check if the prediction matches the ground truth\n",
    "        \n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "151d9116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f74d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The',\n",
       " 'generation_info': {'finish_reason': 'length',\n",
       "  'logprobs': {'content': [{'token': 'The',\n",
       "     'bytes': [84, 104, 101],\n",
       "     'logprob': -0.9243025,\n",
       "     'top_logprobs': [{'token': 'The',\n",
       "       'bytes': [84, 104, 101],\n",
       "       'logprob': -0.9243025},\n",
       "      {'token': 'Answer',\n",
       "       'bytes': [65, 110, 115, 119, 101, 114],\n",
       "       'logprob': -1.0649276},\n",
       "      {'token': 'Z', 'bytes': [90], 'logprob': -2.3774276},\n",
       "      {'token': 'In', 'bytes': [73, 110], 'logprob': -2.5180526},\n",
       "      {'token': 'A', 'bytes': [65], 'logprob': -3.2836776}]}]}},\n",
       " 'type': 'ChatGeneration',\n",
       " 'message': {'content': 'The',\n",
       "  'additional_kwargs': {},\n",
       "  'type': 'ai',\n",
       "  'example': False}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response['generations'][0][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e4720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dda039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_subject = 'business ethics'\n",
    "question = \"Three contrasting tactics that CSO's can engage in to meet their aims are ________ which typically involves research and communication, ________, which may involve physically attacking a company's operations or ________, often involving some form of _______.\"\n",
    "choices = '''A. Non-violent direct action, Violent direct action, Indirect action, Boycott\n",
    "B. Indirect action, Instrumental action, Non-violent direct action, Information campaign\n",
    "C. Indirect action, Violent direct action, Non-violent direct-action Boycott\n",
    "D. Non-violent direct action, Instrumental action, Indirect action, Information campaign'''\n",
    "\n",
    "messages = mmlu_prompt_template.format_messages(\n",
    "    formatted_subject = formatted_subject,\n",
    "    question = question,\n",
    "    choices = choices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over all the rows of the DataFrame\n",
    "for index, row in df_subject.iterrows():\n",
    "    \n",
    "    # Extracting the question from the row\n",
    "    question = row['question']\n",
    "    print(question)\n",
    "\n",
    "    # Extracting the choices from the row\n",
    "    choices = ''\n",
    "    all_choices = row['choices']\n",
    "    for index, choice in enumerate(all_choices):\n",
    "        choices += f'{MMLU_CHOICES[index]}. {choice}\\n'\n",
    "\n",
    "    # Extracting the answer from the row\n",
    "    number_answer = row['answer']\n",
    "    answer = MMLU_CHOICES[number_answer]\n",
    "\n",
    "    # Invoking the LLM to generate a response\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc9f25",
   "metadata": {},
   "source": [
    "Typical advertising regulatory bodies suggest, for example that adverts must not: encourage _________, cause unnecessary ________ or _____, and must not cause _______ offence.\n",
    "\n",
    "A. Unsafe practices, Wants, Fear, Trivial\n",
    "\n",
    "B. Unsafe practices, Distress, Fear, Serious\n",
    "\n",
    "C. Safe practices, Wants, Jealousy, Trivial\n",
    "\n",
    "D. Safe practices, Distress, Jealousy, Serious\n",
    "\n",
    "Answer: B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f79696",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_answer = df_subject.iloc[0]['answer']\n",
    "answer = CHOICES[number_answer]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = ''\n",
    "for index, choice in enumerate(all_choices):\n",
    "    choices += f'{CHOICES[index]}. {choice}\\n'\n",
    "print(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = API_KEYS['OPENAI_API_KEY'])\n",
    "\n",
    "test_prompt = '''The following are questions (with answers) about business ethics.\n",
    "\n",
    "Three contrasting tactics that CSO's can engage in to meet their aims are ________ which typically involves research and communication, ________, which may involve physically attacking a company's operations or ________, often involving some form of _______.\n",
    "A. Non-violent direct action, Violent direct action, Indirect action, Boycott\n",
    "B. Indirect action, Instrumental action, Non-violent direct action, Information campaign\n",
    "C. Indirect action, Violent direct action, Non-violent direct-action Boycott\n",
    "D. Non-violent direct action, Instrumental action, Indirect action, Information campaign\n",
    "'''\n",
    "\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model = 'gpt-3.5-turbo-1106',\n",
    "    logprobs = True,\n",
    "    top_logprobs = 5,\n",
    "    max_tokens = 1,\n",
    "    temperature = 0,\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': test_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_json = json.loads(completion.model_dump_json())\n",
    "test_probs = completion_json['choices'][0]['logprobs']['content'][0]['top_logprobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cafb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_logprobs = {}\n",
    "for prob in test_probs:\n",
    "    token_name = prob['token']\n",
    "    token_logprob = prob['logprob']\n",
    "    generated_logprobs[token_name] = token_logprob\n",
    "generated_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = list(generated_logprobs.values())\n",
    "print(logprobs)\n",
    "\n",
    "def softmax(x):\n",
    "    z = x - max(x)\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator)\n",
    "    softmax = numerator/denominator\n",
    "    return softmax\n",
    "\n",
    "softmax(np.array(list(token_logprobs.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = []\n",
    "for choice in MMLU_CHOICES:\n",
    "    \n",
    "\n",
    "    completion_json['choices'][0]['logprobs']['content'][0]['top_logprobs']['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2569dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.completions.create(\n",
    "    model = 'gpt-3.5-turbo-instruct',\n",
    "    prompt = test_prompt,\n",
    "    logprobs = 5,\n",
    "    max_tokens = 1,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate(messages = [messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4da278",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(result.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8931042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
