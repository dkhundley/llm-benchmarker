{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17958130",
   "metadata": {},
   "source": [
    "# MMLU - Massive Multitask Language Understanding\n",
    "\n",
    "MMLU stands for **Massive Multitask Language Understanding**, and it is perhaps the most popular metric used across model cards to demonstrate a model's performance in terms of knowledge breadth. This benchmark contains a series of scenarios and questions for the LLM to answer across 57 different domains. These domains include STEM, humanities, social sciences, and more. Within each of these domains, there include questions that range from more generalized areas, like history of the topic, and then there are questions that are more specialized in nature or ask \"harder\" questions, like ethical implications.\n",
    "\n",
    "Originally conceived by a team a UC Berkeley in ?, MMLU has evolved into many different flavors, each taking variance on things like prompting style, evaluation codes, or even using a subset of all the questions asked. HuggingFace has [a really great write up](https://github.com/huggingface/blog/blob/main/evaluating-mmlu-leaderboard.md) on all these different variations, and while they all can produce a wide range of differences, the same goal remains: assessing the LLM's breadth of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24612808",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da226943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c719436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading API keys from file (NOT pushed up to GitHub)\n",
    "with open('../keys/api_keys.yaml') as f:\n",
    "    API_KEYS = yaml.safe_load(f)\n",
    "\n",
    "# Statically setting the choices available\n",
    "MMLU_CHOICES = ['A', 'B', 'C', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94934ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    z = x - max(x)\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator)\n",
    "    softmax = numerator/denominator\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03e567",
   "metadata": {},
   "source": [
    "## LLM & LangChain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2ce925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the OpenAI client with LangChain\n",
    "llm = ChatOpenAI(\n",
    "    api_key = API_KEYS['OPENAI_API_KEY'],\n",
    "    model_name = 'gpt-4',\n",
    "    temperature = 0,\n",
    "    max_tokens = 1,\n",
    "    model_kwargs = {\n",
    "        'logprobs': True,\n",
    "        'top_logprobs': 5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0641222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing the original prompt\n",
    "ORIGINAL_MMLU_PROMPT_TEMPLATE = '''The following are questions (with answers) about {formatted_subject}.\n",
    "\n",
    "{question}\n",
    "{choices}\n",
    "'''\n",
    "\n",
    "# Setting a template around the prompt\n",
    "mmlu_prompt_template = HumanMessagePromptTemplate.from_template(ORIGINAL_MMLU_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d367f9",
   "metadata": {},
   "source": [
    "## Loading the MMLU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd64871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Down)loading the MMLU dataset from HuggingFace\n",
    "mmlu_dataset = load_dataset(path = 'cais/mmlu',\n",
    "                            name = 'all',\n",
    "                            cache_dir = '../data/',\n",
    "                            trust_remote_code = True,\n",
    "                            split = 'dev')\n",
    "\n",
    "# Loading the dataset as a Pandas dataframe\n",
    "df_mmlu = mmlu_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788f55f",
   "metadata": {},
   "source": [
    "## Starting With One Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d794629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: moral_disputes\n",
      "Formatted Subject: moral disputes\n"
     ]
    }
   ],
   "source": [
    "# Getting a list of all the subjects\n",
    "subjects = sorted(df_mmlu['subject'].value_counts().keys())\n",
    "\n",
    "# Extracting a single sample subject\n",
    "subject = subjects[42]\n",
    "\n",
    "# Formatting the subject by removing the underscores\n",
    "formatted_subject = subject.replace('_', ' ')\n",
    "\n",
    "print(f'Subject: {subject}')\n",
    "print(f'Formatted Subject: {formatted_subject}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd62dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>According to Metz, what is wrong with conseque...</td>\n",
       "      <td>moral_disputes</td>\n",
       "      <td>[It is unclear as of yet whether or not capita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>A fertilized ovum is also known as</td>\n",
       "      <td>moral_disputes</td>\n",
       "      <td>[a zygote., an embryo., a viability., a blasto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Which of the following is an intrinsic good, a...</td>\n",
       "      <td>moral_disputes</td>\n",
       "      <td>[being in the upper class, the ability to lear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Pence compares six different cases of reproduc...</td>\n",
       "      <td>moral_disputes</td>\n",
       "      <td>[SCNT is not a different kind of reproduction ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Baron admits that the versions of the ticking ...</td>\n",
       "      <td>moral_disputes</td>\n",
       "      <td>[the stupidity of most traditional philosophic...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question         subject  \\\n",
       "155  According to Metz, what is wrong with conseque...  moral_disputes   \n",
       "156                 A fertilized ovum is also known as  moral_disputes   \n",
       "157  Which of the following is an intrinsic good, a...  moral_disputes   \n",
       "158  Pence compares six different cases of reproduc...  moral_disputes   \n",
       "159  Baron admits that the versions of the ticking ...  moral_disputes   \n",
       "\n",
       "                                               choices  answer  \n",
       "155  [It is unclear as of yet whether or not capita...       0  \n",
       "156  [a zygote., an embryo., a viability., a blasto...       0  \n",
       "157  [being in the upper class, the ability to lear...       1  \n",
       "158  [SCNT is not a different kind of reproduction ...       0  \n",
       "159  [the stupidity of most traditional philosophic...       3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering down the full dataset to only include the currently analyzed subject\n",
    "df_subject = df_mmlu[df_mmlu['subject'] == subject]\n",
    "df_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef295fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating containers to hold all boolean \"is correct\" values and probabilities for this subject\n",
    "subject_is_corrects = []\n",
    "subject_probs = []\n",
    "\n",
    "# Iterating over all the rows of the DataFrame\n",
    "for index, row in df_subject.iterrows():\n",
    "    \n",
    "    # Extracting the question from the row\n",
    "    question = row['question']\n",
    "\n",
    "    # Extracting the choices from the row\n",
    "    choices = ''\n",
    "    all_choices = row['choices']\n",
    "    for index, choice in enumerate(all_choices):\n",
    "        choices += f'{MMLU_CHOICES[index]}. {choice}\\n'\n",
    "\n",
    "    # Extracting the answer from the row\n",
    "    number_answer = row['answer']\n",
    "    ground_truth = MMLU_CHOICES[number_answer]\n",
    "\n",
    "    # Setting the variables within the template\n",
    "    messages = mmlu_prompt_template.format_messages(\n",
    "        formatted_subject = formatted_subject,\n",
    "        question = question,\n",
    "        choices = choices\n",
    "    )\n",
    "\n",
    "    # Invoking the LLM to generate a response\n",
    "    llm_response = json.loads(llm.generate(messages = [messages]).json())\n",
    "\n",
    "    # Extracting the prediction from the LLM response\n",
    "    prediction = llm_response['generations'][0][0]['text']\n",
    "\n",
    "    # Extracting the log probs from the LLM response\n",
    "    logprobs = llm_response['generations'][0][0]['generation_info']['logprobs']['content'][0]['top_logprobs']\n",
    "\n",
    "    # Instantiating a dictionary to a direct key-value pair between token name and that token's logprob\n",
    "    token_logprobs = {}\n",
    "\n",
    "    # Iterating over the \"raw\" log probs\n",
    "    for logprob in logprobs:\n",
    "        token_name = logprob['token']\n",
    "        token_logprob = logprob['logprob']\n",
    "        token_logprobs[token_name] = token_logprob\n",
    "\n",
    "    # Instantiating a dictionary to hold the log probs for each of the respective ABCD answers\n",
    "    abcd_logprobs = {}\n",
    "\n",
    "    # Iterating over each of the MMLU choices\n",
    "    for choice in MMLU_CHOICES:\n",
    "\n",
    "        # Checking to see if each ABCD answer is in the current logprobs\n",
    "        if choice in token_logprobs.keys():\n",
    "            \n",
    "            # Adding the appropriate logprob to abcd_logprobs\n",
    "            abcd_logprobs[choice] = token_logprobs[choice]\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Adding a fake penalty logprob since the answer isn't present\n",
    "            abcd_logprobs[choice] = -100\n",
    "\n",
    "    # Setting the softmax values for the ABCD logprobs\n",
    "    abcd_prob_values = softmax(np.array(list(abcd_logprobs.values())))\n",
    "\n",
    "    # Creating a boolean value to check if the prediction matches the ground truth\n",
    "    is_true = ground_truth == prediction\n",
    "    \n",
    "    # Appending the final values\n",
    "    subject_is_corrects.append(is_true)\n",
    "    subject_probs.append(abcd_prob_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66595427",
   "metadata": {},
   "source": [
    "## Running the Full MMLU Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f5f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of all the subjects\n",
    "subjects = sorted(df_mmlu['subject'].value_counts().keys())\n",
    "\n",
    "all_is_corrects = []\n",
    "all_probs = []\n",
    "\n",
    "# Iterating over all the subjects\n",
    "for subject in subjects:\n",
    "\n",
    "    # Formatting the subject by removing the underscores\n",
    "    formatted_subject = subject.replace('_', ' ')\n",
    "\n",
    "    # Instantiating containers to hold all boolean \"is correct\" values and probabilities for this subject\n",
    "    subject_is_corrects = []\n",
    "    subject_probs = []\n",
    "\n",
    "    # Iterating over all the rows of the DataFrame\n",
    "    for index, row in df_subject.iterrows():\n",
    "        \n",
    "        # Extracting the question from the row\n",
    "        question = row['question']\n",
    "\n",
    "        # Extracting the choices from the row\n",
    "        choices = ''\n",
    "        all_choices = row['choices']\n",
    "        for index, choice in enumerate(all_choices):\n",
    "            choices += f'{MMLU_CHOICES[index]}. {choice}\\n'\n",
    "\n",
    "        # Extracting the answer from the row\n",
    "        number_answer = row['answer']\n",
    "        ground_truth = MMLU_CHOICES[number_answer]\n",
    "\n",
    "        # Setting the variables within the template\n",
    "        messages = mmlu_prompt_template.format_messages(\n",
    "            formatted_subject = formatted_subject,\n",
    "            question = question,\n",
    "            choices = choices\n",
    "        )\n",
    "\n",
    "        # Invoking the LLM to generate a response\n",
    "        llm_response = json.loads(llm.generate(messages = [messages]).json())\n",
    "\n",
    "        # Extracting the prediction from the LLM response\n",
    "        prediction = llm_response['generations'][0][0]['text']\n",
    "\n",
    "        # Extracting the log probs from the LLM response\n",
    "        logprobs = llm_response['generations'][0][0]['generation_info']['logprobs']['content'][0]['top_logprobs']\n",
    "\n",
    "        # Instantiating a dictionary to a direct key-value pair between token name and that token's logprob\n",
    "        token_logprobs = {}\n",
    "\n",
    "        # Iterating over the \"raw\" log probs\n",
    "        for logprob in logprobs:\n",
    "            token_name = logprob['token']\n",
    "            token_logprob = logprob['logprob']\n",
    "            token_logprobs[token_name] = token_logprob\n",
    "\n",
    "        # Instantiating a dictionary to hold the log probs for each of the respective ABCD answers\n",
    "        abcd_logprobs = {}\n",
    "\n",
    "        # Iterating over each of the MMLU choices\n",
    "        for choice in MMLU_CHOICES:\n",
    "\n",
    "            # Checking to see if each ABCD answer is in the current logprobs\n",
    "            if choice in token_logprobs.keys():\n",
    "                \n",
    "                # Adding the appropriate logprob to abcd_logprobs\n",
    "                abcd_logprobs[choice] = token_logprobs[choice]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Adding a fake penalty logprob since the answer isn't present\n",
    "                abcd_logprobs[choice] = -100\n",
    "\n",
    "        # Setting the softmax values for the ABCD logprobs\n",
    "        abcd_prob_values = softmax(np.array(list(abcd_logprobs.values())))\n",
    "\n",
    "        # Creating a boolean value to check if the prediction matches the ground truth\n",
    "        is_true = ground_truth == prediction\n",
    "        \n",
    "        # Appending the final values\n",
    "        subject_is_corrects.append(is_true)\n",
    "        subject_probs.append(abcd_prob_values)\n",
    "\n",
    "    # Appending the mean of all the correct subject answers to the full list of correct answers\n",
    "    all_is_corrects.append(np.array(np.mean(subject_is_corrects)))\n",
    "\n",
    "    # Appending all the correlations together\n",
    "    all_probs.append(subject_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dae6213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5543859649122806\n"
     ]
    }
   ],
   "source": [
    "weighted_acc = np.mean(all_is_corrects)\n",
    "print(weighted_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584dc0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42959f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea3aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.46639708e-03, 3.73683562e-44, 9.76271730e-01, 2.02618732e-02]),\n",
       " array([1.00000000e+00, 3.73643148e-44, 3.73643148e-44, 3.73643148e-44]),\n",
       " array([3.73869673e-44, 1.00000000e+00, 3.73869673e-44, 3.73869673e-44]),\n",
       " array([9.99958993e-01, 3.76226296e-44, 2.50712080e-05, 1.59362294e-05]),\n",
       " array([2.03314051e-02, 3.81365273e-44, 4.66091520e-05, 9.79621986e-01])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f74d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response['generations'][0][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e4720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dda039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_subject = 'business ethics'\n",
    "question = \"Three contrasting tactics that CSO's can engage in to meet their aims are ________ which typically involves research and communication, ________, which may involve physically attacking a company's operations or ________, often involving some form of _______.\"\n",
    "choices = '''A. Non-violent direct action, Violent direct action, Indirect action, Boycott\n",
    "B. Indirect action, Instrumental action, Non-violent direct action, Information campaign\n",
    "C. Indirect action, Violent direct action, Non-violent direct-action Boycott\n",
    "D. Non-violent direct action, Instrumental action, Indirect action, Information campaign'''\n",
    "\n",
    "messages = mmlu_prompt_template.format_messages(\n",
    "    formatted_subject = formatted_subject,\n",
    "    question = question,\n",
    "    choices = choices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over all the rows of the DataFrame\n",
    "for index, row in df_subject.iterrows():\n",
    "    \n",
    "    # Extracting the question from the row\n",
    "    question = row['question']\n",
    "    print(question)\n",
    "\n",
    "    # Extracting the choices from the row\n",
    "    choices = ''\n",
    "    all_choices = row['choices']\n",
    "    for index, choice in enumerate(all_choices):\n",
    "        choices += f'{MMLU_CHOICES[index]}. {choice}\\n'\n",
    "\n",
    "    # Extracting the answer from the row\n",
    "    number_answer = row['answer']\n",
    "    answer = MMLU_CHOICES[number_answer]\n",
    "\n",
    "    # Invoking the LLM to generate a response\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc9f25",
   "metadata": {},
   "source": [
    "Typical advertising regulatory bodies suggest, for example that adverts must not: encourage _________, cause unnecessary ________ or _____, and must not cause _______ offence.\n",
    "\n",
    "A. Unsafe practices, Wants, Fear, Trivial\n",
    "\n",
    "B. Unsafe practices, Distress, Fear, Serious\n",
    "\n",
    "C. Safe practices, Wants, Jealousy, Trivial\n",
    "\n",
    "D. Safe practices, Distress, Jealousy, Serious\n",
    "\n",
    "Answer: B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f79696",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_answer = df_subject.iloc[0]['answer']\n",
    "answer = CHOICES[number_answer]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = ''\n",
    "for index, choice in enumerate(all_choices):\n",
    "    choices += f'{CHOICES[index]}. {choice}\\n'\n",
    "print(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = API_KEYS['OPENAI_API_KEY'])\n",
    "\n",
    "test_prompt = '''The following are questions (with answers) about business ethics.\n",
    "\n",
    "Three contrasting tactics that CSO's can engage in to meet their aims are ________ which typically involves research and communication, ________, which may involve physically attacking a company's operations or ________, often involving some form of _______.\n",
    "A. Non-violent direct action, Violent direct action, Indirect action, Boycott\n",
    "B. Indirect action, Instrumental action, Non-violent direct action, Information campaign\n",
    "C. Indirect action, Violent direct action, Non-violent direct-action Boycott\n",
    "D. Non-violent direct action, Instrumental action, Indirect action, Information campaign\n",
    "'''\n",
    "\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model = 'gpt-3.5-turbo-1106',\n",
    "    logprobs = True,\n",
    "    top_logprobs = 5,\n",
    "    max_tokens = 1,\n",
    "    temperature = 0,\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': test_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_json = json.loads(completion.model_dump_json())\n",
    "test_probs = completion_json['choices'][0]['logprobs']['content'][0]['top_logprobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cafb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_logprobs = {}\n",
    "for prob in test_probs:\n",
    "    token_name = prob['token']\n",
    "    token_logprob = prob['logprob']\n",
    "    generated_logprobs[token_name] = token_logprob\n",
    "generated_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = list(generated_logprobs.values())\n",
    "print(logprobs)\n",
    "\n",
    "def softmax(x):\n",
    "    z = x - max(x)\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator)\n",
    "    softmax = numerator/denominator\n",
    "    return softmax\n",
    "\n",
    "softmax(np.array(list(token_logprobs.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = []\n",
    "for choice in MMLU_CHOICES:\n",
    "    \n",
    "\n",
    "    completion_json['choices'][0]['logprobs']['content'][0]['top_logprobs']['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2569dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.completions.create(\n",
    "    model = 'gpt-3.5-turbo-instruct',\n",
    "    prompt = test_prompt,\n",
    "    logprobs = 5,\n",
    "    max_tokens = 1,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate(messages = [messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4da278",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(result.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8931042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
